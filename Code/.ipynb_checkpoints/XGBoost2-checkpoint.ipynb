{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "* Performs xgboost on training data. \n",
    "* Iterates over parameters with cross validation\n",
    "* Currently ignoring date parameters due to large number of factors. Waiting for preprocessing steps to improve. \n",
    "* Warning: takes a long time to cross validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "library(xgboost)\n",
    "library(dplyr)\n",
    "library(Matrix)\n",
    "library(data.table)\n",
    "library(Ckmeans.1d.dp)\n",
    "library(e1071)\n",
    "library(caret)\n",
    "\n",
    "# Set Seed\n",
    "set.seed(1066)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "# remove id and date_first_booking as they are not relevant\n",
    "# CURRENTLY REMOVESDATE PARAMETERS AS WELL\n",
    "tr <- readRDS(\"../Data/train_users_2_tr.RDS\") %>%\n",
    "    na.omit() %>%\n",
    "    select(-c(id, date_first_booking)) %>%\n",
    "    select(-c(date_account_created, date_first_booking, first_browser)) ## Removing more complex factors\n",
    "\n",
    "ts <- readRDS(\"../Data/train_users_2_ts.RDS\") %>%\n",
    "    na.omit() %>%\n",
    "    select(-c(id, date_first_booking)) %>%\n",
    "    select(-c(date_account_created, first_browser)) ## Removing more complex factors\n",
    "\n",
    "test <- read.csv(\"../test_users.csv\") %>%\n",
    "    na.omit() %>%\n",
    "    select(-id, -date_first_booking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>93985</li>\n",
       "\t<li>75</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 93985\n",
       "\\item 75\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 93985\n",
       "2. 75\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 93985    75"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encoding  \n",
    "# https://cran.r-project.org/web/packages/xgboost/vignettes/discoverYourData.html\n",
    "tr <- data.table(tr, keep.rownames = F)\n",
    "sparse_tr <- sparse.model.matrix(country_destination~. -1, data = tr)\n",
    "dim(sparse_tr)\n",
    "\n",
    "ts <- data.table(ts, keep.rownames = F)\n",
    "sparse_ts <- sparse.model.matrix(country_destination~. -1, data = ts)\n",
    "dim(sparse_ts)\n",
    "\n",
    "test <- data.table(test, keep.rownames = F)\n",
    "sparse_test <- sparse.model.matrix(~., data = test)\n",
    "dim(sparse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameter search using Cross validation\n",
    "# http://stats.stackexchange.com/questions/171043/how-to-tune-hyperparameters-of-xgboost-trees\n",
    "# Currently using low number of rounds to test\n",
    "\n",
    "# set up the cross-validated hyper-parameter search\n",
    "xgb_grid_1 = expand.grid(\n",
    "    nrounds = 50,                   # Iterations building each XGB model\n",
    "    max_depth = c(2, 4, 6, 8, 10),   # Maximum tree depth\n",
    "    eta = c(0.01, 0.001, 0.0001),    # Learning rate\n",
    "    gamma = 1,                       # Min loss reduction required to make a partition on leaf node [0:inf]\n",
    "    colsample_bytree = 0.3,          # proportion of features used in each tree\n",
    "    min_child_weight = 1\n",
    ")\n",
    "\n",
    "# trainControl creates settings for caret::train\n",
    "xgb_trcontrol_1 = trainControl(\n",
    "    method = \"cv\",          # Cross validation\n",
    "    number = 5,             # number of folds\n",
    "    verboseIter = TRUE,\n",
    "    returnData = FALSE,\n",
    "    returnResamp = \"all\",   # How many summary stats to save # save losses across all models\n",
    "    allowParallel = TRUE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: plyr\n",
      "------------------------------------------------------------------------------\n",
      "You have loaded plyr after dplyr - this is likely to cause problems.\n",
      "If you need functions from both plyr and dplyr, please load plyr first, then dplyr:\n",
      "library(plyr); library(dplyr)\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "Attaching package: 'plyr'\n",
      "\n",
      "The following objects are masked from 'package:dplyr':\n",
      "\n",
      "    arrange, count, desc, failwith, id, mutate, rename, summarise,\n",
      "    summarize\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train XGboost\n",
    "# \"Kappa\" metric used for evaluation\n",
    "xgb_train_1 = train(\n",
    "    x = sparse_tr,\n",
    "    y = tr$country_destination,\n",
    "    trControl = xgb_trcontrol_1,\n",
    "    tuneGrid = xgb_grid_1,\n",
    "    method = \"xgbTree\", \n",
    "    metric = \"Kappa\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluating importance of features to the model\n",
    "importance <- xgb.importance(sparse_tr@Dimnames[[2]], \n",
    "                             model = xgb_1, \n",
    "                             data = sparse_tr, \n",
    "                             label = tr$country_destination)\n",
    "xgb.plot.importance(importance_matrix = importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scatter plot of the Kappa against max_depth and eta\n",
    "ggplot(xgb_train_1$results, aes(x = as.factor(eta), y = max_depth, size = Kappa, color = Kappa)) + \n",
    "    geom_point() + \n",
    "    theme_bw() + \n",
    "    scale_size_continuous(guide = \"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate performance on test set\n",
    "pred <- predict(xgb_train_1, sparse_ts)\n",
    "dim(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate predictions on competition test set. \n",
    "pred <- predict(xgb_train_1, sparse_test)\n",
    "# compare prediction to results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
